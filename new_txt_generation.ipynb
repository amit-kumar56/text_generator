{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from numpy import array\n",
    "import itertools\n",
    "import keras\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,Embedding\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense,LSTM,GRU\n",
    "from keras.layers import Activation, Dropout,BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras import applications\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import applications \n",
    "from keras.utils.np_utils import to_categorical \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg\n",
    "import math \n",
    "import datetime\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import requests,io,random,re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import ssl\n",
    "import random\n",
    "import cv2 \n",
    "import numpy as np\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx='Please note that we choose to Keras sequential model here since all the layers in the model only have single input and produce single output. In case you want to retrieve and reuse the states from stateful RNN layer, you might want to build your model with Keras functional API or model subclassing. Please check Keras RNN guide for more details.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394\n"
     ]
    }
   ],
   "source": [
    "text1 = open(text, 'rb').read()\n",
    "text1=text1.decode(encoding='utf-8')\n",
    "#text1=tx\n",
    "print ('Length of text:' ,len(text1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=text1[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "print(text1[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "num_unique_char=set(text1)\n",
    "num_unique_char=sorted(num_unique_char)\n",
    "print(len(num_unique_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_to_ind={c:i for i,c in enumerate(num_unique_char)}\n",
    "ind_to_ch=np.array(num_unique_char)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " : 0\n",
      "  : 1\n",
      "! : 2\n",
      "' : 3\n",
      ", : 4\n",
      "- : 5\n",
      ". : 6\n",
      ": : 7\n",
      "; : 8\n",
      "? : 9\n",
      "A : 10\n",
      "B : 11\n",
      "C : 12\n",
      "F : 13\n",
      "I : 14\n",
      "L : 15\n",
      "M : 16\n",
      "N : 17\n",
      "O : 18\n",
      "R : 19\n",
      "S : 20\n",
      "T : 21\n",
      "V : 22\n",
      "W : 23\n",
      "Y : 24\n",
      "a : 25\n",
      "b : 26\n",
      "c : 27\n",
      "d : 28\n",
      "e : 29\n",
      "f : 30\n",
      "g : 31\n",
      "h : 32\n",
      "i : 33\n",
      "j : 34\n",
      "k : 35\n",
      "l : 36\n",
      "m : 37\n",
      "n : 38\n",
      "o : 39\n",
      "p : 40\n",
      "r : 41\n",
      "s : 42\n",
      "t : 43\n",
      "u : 44\n",
      "v : 45\n",
      "w : 46\n",
      "y : 47\n",
      "z : 48\n"
     ]
    }
   ],
   "source": [
    "for ch in (num_unique_char):\n",
    "    print(ch ,':', ch_to_ind[ch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int=np.array([ch_to_ind[c] for c in text1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Befor [13 33 41 42 43  1 12 33 43 33 48 29 38]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (text1[:20], text_as_int[:13])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=100\n",
    "num_batch=len(text1)//(max_len+1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for i in ch_dataset.take(8):\n",
    "    print(ind_to_ch[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = ch_dataset.batch(max_len+1, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F' 'i' 'r' 's' 't' ' ' 'C' 'i' 't' 'i' 'z' 'e' 'n' ':' '\\n' 'B' 'e' 'f'\n",
      " 'o' 'r' 'e' ' ' 'w' 'e' ' ' 'p' 'r' 'o' 'c' 'e' 'e' 'd' ' ' 'a' 'n' 'y'\n",
      " ' ' 'f' 'u' 'r' 't' 'h' 'e' 'r' ',' ' ' 'h' 'e' 'a' 'r' ' ' 'm' 'e' ' '\n",
      " 's' 'p' 'e' 'a' 'k' '.' '\\n' '\\n' 'A' 'l' 'l' ':' '\\n' 'S' 'p' 'e' 'a'\n",
      " 'k' ',' ' ' 's' 'p' 'e' 'a' 'k' '.' '\\n' '\\n' 'F' 'i' 'r' 's' 't' ' ' 'C'\n",
      " 'i' 't' 'i' 'z' 'e' 'n' ':' '\\n' 'Y' 'o' 'u' ' ']\n",
      "['a' 'r' 'e' ' ' 'a' 'l' 'l' ' ' 'r' 'e' 's' 'o' 'l' 'v' 'e' 'd' ' ' 'r'\n",
      " 'a' 't' 'h' 'e' 'r' ' ' 't' 'o' ' ' 'd' 'i' 'e' ' ' 't' 'h' 'a' 'n' ' '\n",
      " 't' 'o' ' ' 'f' 'a' 'm' 'i' 's' 'h' '?' '\\n' '\\n' 'A' 'l' 'l' ':' '\\n'\n",
      " 'R' 'e' 's' 'o' 'l' 'v' 'e' 'd' '.' ' ' 'r' 'e' 's' 'o' 'l' 'v' 'e' 'd'\n",
      " '.' '\\n' '\\n' 'F' 'i' 'r' 's' 't' ' ' 'C' 'i' 't' 'i' 'z' 'e' 'n' ':'\n",
      " '\\n' 'F' 'i' 'r' 's' 't' ',' ' ' 'y' 'o' 'u' ' ' 'k']\n",
      "['n' 'o' 'w' ' ' 'C' 'a' 'i' 'u' 's' ' ' 'M' 'a' 'r' 'c' 'i' 'u' 's' ' '\n",
      " 'i' 's' ' ' 'c' 'h' 'i' 'e' 'f' ' ' 'e' 'n' 'e' 'm' 'y' ' ' 't' 'o' ' '\n",
      " 't' 'h' 'e' ' ' 'p' 'e' 'o' 'p' 'l' 'e' '.' '\\n' '\\n' 'A' 'l' 'l' ':'\n",
      " '\\n' 'W' 'e' ' ' 'k' 'n' 'o' 'w' \"'\" 't' ',' ' ' 'w' 'e' ' ' 'k' 'n' 'o'\n",
      " 'w' \"'\" 't' '.' '\\n' '\\n' 'F' 'i' 'r' 's' 't' ' ' 'C' 'i' 't' 'i' 'z' 'e'\n",
      " 'n' ':' '\\n' 'L' 'e' 't' ' ' 'u' 's' ' ' 'k' 'i']\n",
      "['l' 'l' ' ' 'h' 'i' 'm' ',' ' ' 'a' 'n' 'd' ' ' 'w' 'e' \"'\" 'l' 'l' ' '\n",
      " 'h' 'a' 'v' 'e' ' ' 'c' 'o' 'r' 'n' ' ' 'a' 't' ' ' 'o' 'u' 'r' ' ' 'o'\n",
      " 'w' 'n' ' ' 'p' 'r' 'i' 'c' 'e' '.' '\\n' 'I' 's' \"'\" 't' ' ' 'a' ' ' 'v'\n",
      " 'e' 'r' 'd' 'i' 'c' 't' '?' '\\n' '\\n' 'A' 'l' 'l' ':' '\\n' 'N' 'o' ' '\n",
      " 'm' 'o' 'r' 'e' ' ' 't' 'a' 'l' 'k' 'i' 'n' 'g' ' ' 'o' 'n' \"'\" 't' ';'\n",
      " ' ' 'l' 'e' 't' ' ' 'i' 't' ' ' 'b' 'e' ' ' 'd']\n",
      "['o' 'n' 'e' ':' ' ' 'a' 'w' 'a' 'y' ',' ' ' 'a' 'w' 'a' 'y' '!' '\\n' '\\n'\n",
      " 'S' 'e' 'c' 'o' 'n' 'd' ' ' 'C' 'i' 't' 'i' 'z' 'e' 'n' ':' '\\n' 'O' 'n'\n",
      " 'e' ' ' 'w' 'o' 'r' 'd' ',' ' ' 'g' 'o' 'o' 'd' ' ' 'c' 'i' 't' 'i' 'z'\n",
      " 'e' 'n' 's' '.' '\\n' '\\n' 'F' 'i' 'r' 's' 't' ' ' 'C' 'i' 't' 'i' 'z' 'e'\n",
      " 'n' ':' '\\n' 'W' 'e' ' ' 'a' 'r' 'e' ' ' 'a' 'c' 'c' 'o' 'u' 'n' 't' 'e'\n",
      " 'd' ' ' 'p' 'o' 'o' 'r' ' ' 'c' 'i' 't' 'i']\n",
      "['z' 'e' 'n' 's' ',' ' ' 't' 'h' 'e' ' ' 'p' 'a' 't' 'r' 'i' 'c' 'i' 'a'\n",
      " 'n' 's' ' ' 'g' 'o' 'o' 'd' '.' '\\n' 'W' 'h' 'a' 't' ' ' 'a' 'u' 't' 'h'\n",
      " 'o' 'r' 'i' 't' 'y' ' ' 's' 'u' 'r' 'f' 'e' 'i' 't' 's' ' ' 'o' 'n' ' '\n",
      " 'w' 'o' 'u' 'l' 'd' ' ' 'r' 'e' 'l' 'i' 'e' 'v' 'e' ' ' 'u' 's' ':' ' '\n",
      " 'i' 'f' ' ' 't' 'h' 'e' 'y' '\\n' 'w' 'o' 'u' 'l' 'd' ' ' 'y' 'i' 'e' 'l'\n",
      " 'd' ' ' 'u' 's' ' ' 'b' 'u' 't' ' ' 't' 'h']\n",
      "['e' ' ' 's' 'u' 'p' 'e' 'r' 'f' 'l' 'u' 'i' 't' 'y' ',' ' ' 'w' 'h' 'i'\n",
      " 'l' 'e' ' ' 'i' 't' ' ' 'w' 'e' 'r' 'e' '\\n' 'w' 'h' 'o' 'l' 'e' 's' 'o'\n",
      " 'm' 'e' ',' ' ' 'w' 'e' ' ' 'm' 'i' 'g' 'h' 't' ' ' 'g' 'u' 'e' 's' 's'\n",
      " ' ' 't' 'h' 'e' 'y' ' ' 'r' 'e' 'l' 'i' 'e' 'v' 'e' 'd' ' ' 'u' 's' ' '\n",
      " 'h' 'u' 'm' 'a' 'n' 'e' 'l' 'y' ';' '\\n' 'b' 'u' 't' ' ' 't' 'h' 'e' 'y'\n",
      " ' ' 't' 'h' 'i' 'n' 'k' ' ' 'w' 'e' ' ' 'a']\n",
      "['r' 'e' ' ' 't' 'o' 'o' ' ' 'd' 'e' 'a' 'r' ':' ' ' 't' 'h' 'e' ' ' 'l'\n",
      " 'e' 'a' 'n' 'n' 'e' 's' 's' ' ' 't' 'h' 'a' 't' '\\n' 'a' 'f' 'f' 'l' 'i'\n",
      " 'c' 't' 's' ' ' 'u' 's' ',' ' ' 't' 'h' 'e' ' ' 'o' 'b' 'j' 'e' 'c' 't'\n",
      " ' ' 'o' 'f' ' ' 'o' 'u' 'r' ' ' 'm' 'i' 's' 'e' 'r' 'y' ',' ' ' 'i' 's'\n",
      " ' ' 'a' 's' ' ' 'a' 'n' '\\n' 'i' 'n' 'v' 'e' 'n' 't' 'o' 'r' 'y' ' ' 't'\n",
      " 'o' ' ' 'p' 'a' 'r' 't' 'i' 'c' 'u' 'l' 'a']\n"
     ]
    }
   ],
   "source": [
    "for i in seq.take(8):\n",
    "    print(ind_to_ch[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you k\n",
      "now Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us ki\n",
      "ll him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be d\n",
      "one: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citi\n",
      "zens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but th\n",
      "e superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we a\n",
      "re too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particula\n"
     ]
    }
   ],
   "source": [
    "for i in seq.take(8):\n",
    "    print(''.join(ind_to_ch[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n",
      "'zens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but th'\n",
      "'e superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we a'\n",
      "'re too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particula'\n"
     ]
    }
   ],
   "source": [
    "for i in seq.take(8):\n",
    "    print(repr(''.join(ind_to_ch[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = seq.map(split_input_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(ind_to_ch[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(ind_to_ch[target_example.numpy()])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE=64\n",
    "BUFFER_SIZE=1000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 13 ('F')\n",
      "  expected output: 33 ('i')\n",
      "Step    1\n",
      "  input: 33 ('i')\n",
      "  expected output: 41 ('r')\n",
      "Step    2\n",
      "  input: 41 ('r')\n",
      "  expected output: 42 ('s')\n",
      "Step    3\n",
      "  input: 42 ('s')\n",
      "  expected output: 43 ('t')\n",
      "Step    4\n",
      "  input: 43 ('t')\n",
      "  expected output: 1 (' ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(ind_to_ch[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(ind_to_ch[target_idx])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(num_unique_char)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size =vocab_size,\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (64, None, 256)           512000    \n",
      "_________________________________________________________________\n",
      "gru_12 (GRU)                 (64, None, 1024)          3935232   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (64, None, 49)            50225     \n",
      "=================================================================\n",
      "Total params: 4,497,457\n",
      "Trainable params: 4,497,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batchc_predictions = rnn(input_example_batch)\n",
    "    print(example_batchc_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (64, None, 256)           512000    \n",
      "_________________________________________________________________\n",
      "gru_12 (GRU)                 (64, None, 1024)          3935232   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (64, None, 49)            50225     \n",
      "=================================================================\n",
      "Total params: 4,497,457\n",
      "Trainable params: 4,497,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "logits should be a matrix, got shape []\n\t [[{{node Multinomial}}]] [Op:Multinomial]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-60a341594c52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msampled_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_batch_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msampled_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/heroo/snap/jupyter/common/lib/python3.7/site-packages/tensorflow/python/ops/random_ops.py\u001b[0m in \u001b[0;36mcategorical\u001b[0;34m(logits, num_samples, dtype, seed, name)\u001b[0m\n\u001b[1;32m    442\u001b[0m   \"\"\"\n\u001b[1;32m    443\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"categorical\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmultinomial_categorical_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/heroo/snap/jupyter/common/lib/python3.7/site-packages/tensorflow/python/ops/random_ops.py\u001b[0m in \u001b[0;36mmultinomial_categorical_impl\u001b[0;34m(logits, num_samples, dtype, seed)\u001b[0m\n\u001b[1;32m    450\u001b[0m   \u001b[0mseed1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m   return gen_random_ops.multinomial(\n\u001b[0;32m--> 452\u001b[0;31m       logits, num_samples, seed=seed1, seed2=seed2, output_dtype=dtype)\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/heroo/snap/jupyter/common/lib/python3.7/site-packages/tensorflow/python/ops/gen_random_ops.py\u001b[0m in \u001b[0;36mmultinomial\u001b[0;34m(logits, num_samples, seed, seed2, output_dtype, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/heroo/snap/jupyter/common/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6651\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/snap/jupyter/6/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: logits should be a matrix, got shape []\n\t [[{{node Multinomial}}]] [Op:Multinomial]"
     ]
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BatchDataset' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-194-95cfed60acec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/heroo/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/heroo/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/heroo/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/heroo/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/heroo/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 'Got tensor with shape: %s' % str(shape))\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BatchDataset' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "rnn.fit(dataset,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
